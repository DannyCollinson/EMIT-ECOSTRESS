{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMIT-ECOSTRESS Patch-to-Pixel Model Training Demo Notebook\n",
    "This notebook will walk you through how we use our patch-to-pixel models for prediction of the number of standard deviations from the mean temperature that a given pixel is as measured by ECOSTRESS from the reflectance spectra of that pixel and possibly others surrounding it as measured by EMIT.\n",
    "\n",
    "To start, specify the absolute path to the project directory on your system where it says `project_path`. Then for the `base_data_path`, specify the relative path from the `project_path` that points to where the dataset is being stored on your system. The dataset should be organized as downloaded from the emit-ecostress Google Drive and as described in the `README` for this project. If this is the case, the `base_data_path` should not have to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = 'C:\\\\Users\\\\danny\\\\Documents_Local\\\\Caltech_2023-2024\\\\CS101'\n",
    "\n",
    "import os\n",
    "base_data_path = os.path.join('Data', 'Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the Model\n",
    "We have to make at least four choices when we set up each problem.\n",
    "1. What type of input to we want to use? Potential options are \"raw\", \"PCA\", or \"AE\", which stands for AutoEncoder.\n",
    "2. What dimension of that input do we want to use? Raw and PCA data can be anywhere from 0- to 244-dimensional, and autoencoder data can be downloaded from the team's Google Drive with dimensions 2, 3, 4, 5, 8, 16, 24, and 32, and you can train your own autoencoder to get additional dimensions as well.\n",
    "3. What \"radius\" of spectral information from other pixels do we want to consider around each pixel when we are predicting its temperature? This number can be from 0 to 191, although we recommend keeping it to 32 and below. To explain what we mean by \"radius\", consider that radius 0 means that we only consider the spectral information for the pixel we are predicting. Radius 1 means that we use all of the spectral information from the 8 adjacent pixels as well. Radius 2 means that we also use the spectral information of the pixels adjacent to the radius 1 pixels. In other words, radius N means that we are using spectral information from a (2N + 1)x(2N + 1) grid of pixels to predict the temperature at that pixel.\n",
    "4. What type of model do we want to use? Here, the potential options are \"linear\", \"mini\", \"small\", \"large\", \"attention\", and \"transformer\". The linear option is a neural network with no hidden layers; the mini has one hidden layer with 4 units; the small has three hidden layers with 32, 16, and 8 units; and the large has six hidden layers with 512, 256, 128, 64, 32, and 16 units. The attention model uses a self-attention block followed by a linear output layer, and the transformer option uses a full transformer.\n",
    "\n",
    "There are several other optional choices, which include the batch sizes, the number of epochs to train for, the dropout rate of the neurons in the model, and the learning rate of the optimizer, but the default choices should work reasonably well in most circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running using cuda\n",
      "\n",
      "radius=0, n_dimensions=2\n",
      "LinearModel(\n",
      "  (linear_output): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 000:     Train (RMSE, K):  0.88024,   2.611   \t Val (RMSE, K):  0.9337,  2.5027   \tLR:  0.001\tTime: 18.8\n",
      "Epoch 001:     Train (RMSE, K):  0.69255,  2.0543   \t Val (RMSE, K):  0.79792,  2.1388   \tLR:  0.001\tTime: 43.3\n"
     ]
    }
   ],
   "source": [
    "# specify these!\n",
    "input_type = 'PCA'\n",
    "n_dimensions = 2\n",
    "radius = 0\n",
    "model_type = 'linear'\n",
    "\n",
    "# optional parameters\n",
    "settings = {\n",
    "    'train_batch_size': 256,\n",
    "    'val_batch_size': 1024,\n",
    "    'n_epochs': 10,\n",
    "    'dropout_rate': 0.0,\n",
    "    'learning_rate': 0.001,\n",
    "}\n",
    "\n",
    "# this runs the training!\n",
    "import sys\n",
    "sys.path.append(os.path.join(project_path, 'EMIT-ECOSTRESS', 'modules'))\n",
    "import utils.run_p2p\n",
    "train_loss, val_loss, stats, train_loss_array, val_loss_array = (\n",
    "    utils.run_p2p.train_patch_to_pixel(\n",
    "        project_path,\n",
    "        base_data_path,\n",
    "        input_type,\n",
    "        n_dimensions,\n",
    "        radius,\n",
    "        model_type,\n",
    "        **settings,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try different models and parameters and try to get below 0.5 validation RMSE!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs101",
   "language": "python",
   "name": "cs101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
